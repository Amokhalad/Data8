{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "%matplotlib inline\n",
    "path_data = '../../../assets/data/'\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- I don't quite get the regression effect. Why does it happen?\n",
    "- Why when $x$ and $y$ are measured in standard units, the regression line passes through the origin?\n",
    "- are all residual plots centered around 0?\n",
    "- It says that residuals plots show no trend. But it sometimes shows a pattern. What is the difference between trend and pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Prediction\n",
    "Terminology:\n",
    "- **regression:** \n",
    "- **ecological correlations:** correlations based on aggregates and averages. (must be interpreted with care)\n",
    "- **Heteroscedasticity**: uneven spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 15.1 Correlation\n",
    "The *correlation coefficient* measures the strength of the **linear** relationship, and only linear association, between two variables. Graphically, it measures how clustered the scatter diagram is around a straight line.\n",
    "\n",
    "Note that outliers can have a big effect on correlation.\n",
    "\n",
    "Some facts about correlation coeff. r.:\n",
    "- $r$ is a number between $-1$ and $1$\n",
    "- $r$ measures the extent to which the scatter plot clusters around a straight line.\n",
    "- r = 1 if the scatter diagram is perfectly straight line sloping upwards, and r = -1 if the scatter diagram is a perfect straight line sloping downwards.\n",
    "\n",
    "\n",
    "##### Calculating $r$\n",
    "the mathematical basis for this is out of scope, but the formula for r is:\n",
    "\n",
    "**$r$ is the average of the products of the two variables, when both variables are measured in standard units**\n",
    "\n",
    "##### Properties of $r$\n",
    "- $r$ is unit-less. This is because $r$ is based on standard units.\n",
    "- $r$ is unaffected by changing the units on either axis. This too is because r is based on standard units.\n",
    "- $r$ is unaffected by switching the axes. Algebraically, this is because the product of standard units does not depend on which variable is called $x$ and which is $y$. Geometrically, switching axes, reflects the scatter plot about the $y = x$, but does not change the amount of clustering nor the sign of the association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2 The Regression Line\n",
    "For a *Football shaped scatter plot;* when $r$ is close to 1, the scatter plot, the 45 degree line, and the regression line are all very close to each other. But for more moderate values of $r$, the regression line is noticeably flatter.\n",
    "\n",
    "#### The Regression Effect\n",
    "With the regression line, it gives us predications that is somewhat closer to the average than the points that were used to make the prediction. This is called \"*regression to the mean*\" and it is how the name *regression arises.*\n",
    "\n",
    "In general, individuals who are away from average on one variable are expected to be not quite as far away from average on the other. This is called the *regression effect.*\n",
    "\n",
    "\n",
    "#### The Equation of the Regression Line\n",
    "When $x$ and $y$ are measured in standard units, the regression line for predicting $y$ based on $x$ has slope $r$ and passes through the origin. This the equation of the regression line is:\n",
    "\n",
    "$$\n",
    "\\text{estimate of } y = r \\cdot x\n",
    "$$\n",
    "\n",
    "The slope and intercept of the regression line in original units is:\n",
    "\n",
    "$$\n",
    "\\text{slope} = r \\cdot \\frac{\\text{SD of }y}{\\text{SD of }x}\n",
    "$$\n",
    "$$\n",
    "\\text{intercept} = \\overline{y} - m\\overline{x}\n",
    "$$\n",
    "\n",
    "A surprising mathematical fact is that no matter what the shape of the scatter plot, the same equation gives the \"best\" among all straight lines. That's the topic of the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3 The Method of Least Squares\n",
    "The purpose of the line is to *predict* or *estimate* values of $y$, but estimates aren't perfect, each one is off the true value by an *error*. A reasonable criterion for a line to be the \"best\" is for it to have the smallest possible overall error among all straight lines.\n",
    "\n",
    "#### Root Mean Squared Error\n",
    "The method of developing a way to measure the rough size of the error is exactly how we develop the SD.\n",
    "\n",
    "We take the mean of the square errors to avoid cancellation when measuring the rough size of the errors, which will give us a measure of roughly how big the squared errors are, but as noted in finding the SD, the units are hard to interpret, so we take the square root. This yields the root mean square error (rmse). Which is the same units as the variable being predict and therefore much easier to understand.\n",
    "\n",
    "A remarkable fact of mathematics is that **for any shaped scatter plot, the regression line is the unique straight line that minimized the mean squared error of estimation among all straight lines.**\n",
    "\n",
    "\n",
    "#### Numerical Optimization\n",
    "The proof of the statement above requires abstract mathematics, by we have python to confirm the statement above.\n",
    "\n",
    "First note that minimizing the root mean squared error minimized the mean squared error, the root makes no difference ti the minimization, so we'll save a step of computation and just minimize the mean squared error (mse).\n",
    "\n",
    "Since the predication depends on the slope $m$ and intercept $b$:\n",
    "$$\n",
    "\\text{prediction} = mx + b\n",
    "$$\n",
    "we can write a function that computes the (mse) that takes in `slope` and `intercept`, and through trial and error, it finds the slope and intercept that minimized the returned value of that function.\n",
    "\n",
    "This function that performs the trial and error is called `minimize`, which follows the changes that lead to incrementally lower output values. The input to `minimize` is a function that itself takes numerical arguments and returns a numerical value.\n",
    "\n",
    "`minimize` will return an array where each element of the array corresponds to the argument of the inputted function such that it minimizes the output of the inputted function.\n",
    "\n",
    "#### The Least Squares Line\n",
    "Therefore, we have found that the regression line minimized the mean squared error, and that minimizing the mean squared error gives us the regression line, **for any shaped scatter plot.**\n",
    "\n",
    "This is why the regression line is sometimes called the \"least squares line.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4 Least Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scatter plots with a non-linear association, it sometimes better to fit a curve than a straight line.\n",
    "\n",
    "#### Nonlinear Regression\n",
    "For a quadratic relation on a scatter plot, we can find the best quadratic function among all quadratic functions just as easy as it was with linear relations using `minimize`. Recall a quadratic function has the form:\n",
    "\n",
    "$$\n",
    "f(x) = ax^2 + bx + c\n",
    "$$\n",
    "for constants $a, b, \\text{and } c$.\n",
    "\n",
    "Therefore we can write a function that finds the mse, where the fitted values are now based on a quadratic function instead of linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_mse(a, b, c):\n",
    "    x = ... # tbl.column('x column')\n",
    "    y = ... # tbl.column('y column')\n",
    "    fitted = a*(x**2) + b*x + c\n",
    "    return np.mean((y - fitted)**2)\n",
    "\n",
    "# best = minimize(quadratic_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5 Visual Diagnostics\n",
    "\n",
    "when using linear regression, we can see how well this method of estimation performs, we must measure how far off the estimates are form the actual values. These differences are called *residuals.*\n",
    "\n",
    "$$\n",
    "\\text{residual} = \\text{observed value} - \\text{regression estimate} \n",
    "$$\n",
    "A residual is what's left over – the residue – after estimation.\n",
    "\n",
    "It is helpful to start visualization, a *residual plot* can be drawn by plotting the residuals against the predictor variable.\n",
    "\n",
    "\n",
    "#### Regression Diagnostics\n",
    "The residual plot of a good regression shows no pattern. The residuals look about the same, above and below the horizontal line at 0, across the range of the predictor variable (x).\n",
    "\n",
    "\n",
    "#### Detecting Nonlinearity\n",
    "While you can usually spot nonlinearity by just drawing a scatter plot of the data, often however, it is easier to spot nonlinearity in a residual plot than in the original plot. This is because residual plot allows us to zoom in on the errors and hence makes it easier to spot patterns.\n",
    "\n",
    "**When a residual plot shows a pattern, there may be a non-linear relation between the variables.**\n",
    "\n",
    "#### Detecting Heteroscedasticity.\n",
    "The meaning of Heteroscedasticity is *\"uneven spread\"*.\n",
    "\n",
    "Take the mpg vs acceleration residual plot of hybrid cars for example.\n",
    "\n",
    "- .\n",
    "  - .\n",
    "    - ![](images/mpg-vs-accel-residual-plot.png)  \n",
    "\n",
    "\n",
    "Notice how the residual plot flares out towards the low end of the accelerations.\n",
    "\n",
    "**If the residual plot shows uneven variation about the horizontal line at 0, the regression estimates are not equally accurate across the range of the predictor variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.6 Numerical Diagnostics\n",
    "\n",
    "#### Residual Plots Show No Trend\n",
    "**for ever linear regression, whether good or bad, the residual plot shows no trend. overall, it is flat, In Other words, the residuals and the predictor variable are uncorrelated (the correlation coeff is 0).**\n",
    "\n",
    "A trend means the general direction of a graph. Residual plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
