{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17. Classification\n",
    "Classification is about learning how to make predictions from past examples. In a classification task, each individual or situation where we'd like to make a prediction is called an ***observation***. Each observation has multiple ***attributes***, which are known. Also, each observation has a ***class***, which is the answer to the question we care about (for example, fraudulent or not, or voting for you or not).\n",
    "\n",
    "Amazon trying determine which orders are fraudulent: When a customer makes a new order, we do not observe whether it is fraudulent, but we do observe its attributes, and we will try to predict its class using those attributes.\n",
    "\n",
    "Classification requires data.  It involves looking for patterns, and to find patterns, you need data.  That's where the data science comes in.  In particular, we're going to assume that we have access to *training data*: a bunch of observations, where we know the class of each observation.  The collection of these pre-classified observations is also called a training set.  A classification algorithm is going to analyze the training set, and then come up with a classifier: an algorithm for predicting the class of future observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.1 Nearest Neighbors\n",
    "A surprisingly effective algorithm for classification is called the *nearest neighbor classification*. The main idea is that you make your classification based on the class of the observation closest to the individual or situation you're trying to predict.\n",
    "\n",
    "The decision boundary two points is not always clean. With decision boundaries that aren't so obvious, there is a simple generalization of the nearest neighbor classifier, called the ***k-Nearest Neighbors***, where we take the majority class for the kth points in our training set as as the class for the point we are trying to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.2 Training and Testing\n",
    "\n",
    "When we computed confidence intervals for numerical parameters, we wanted to have many new random samples from a population, but we only had access to a single sample. We solved that problem by taking bootstrap resamples from our sample.\n",
    "\n",
    "We will use an analogous idea to test our classifier. We will create two samples out of the original training set, use one of the samples as our training set, and the other one for testing.\n",
    "\n",
    "So we will have three groups of individuals:\n",
    "- a training set on which we can do any amount of exploration to build our classifier;\n",
    "- a separate testing set on which to try out our classifier and see what fraction of times it classifies correctly;\n",
    "- the underlying population of individuals for whom we don't know the true classes; the hope is that our classifier will succeed about as well for these individuals as it did for our testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.3 Rows of Tables\n",
    "Rows are in general **not arrays**, as their elements can be of different types. For example, some of the elements of the row above are strings (like 'abnormal') and some are numerical. So the row canâ€™t always be converted into an array. Rows whose elements are all numerical (or all strings) can be converted to arrays by the `np.array` function.\n",
    "\n",
    "If you use apply without specifying a column label, then the entire row is passed to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 17.4 Implementing The Classifier\n",
    "This is a general phenomenom in classification. Each attribute can potentially give you new information, so more attributes sometimes helps you build a better classifier.\n",
    "\n",
    "##### A Plan for the Implementation\n",
    "It's time to write some code to implement the classifier.  The input is a `point` that we want to classify.  The classifier works by finding the $k$ nearest neighbors of `point` from the training set.  So, our approach will go like this:\n",
    "\n",
    "1. Find the closest $k$ neighbors of `point`, i.e., the $k$ wines from the training set that are most similar to `point`.\n",
    "\n",
    "2. Look at the classes of those $k$ neighbors, and take the majority vote to find the most-common class of wine.  Use that as our predicted class for `point`.\n",
    "\n",
    "So that will guide the structure of our Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ________________________________________\n",
    "# STEP 1:\n",
    "def distance(point1, point2):\n",
    "    \"\"\"Returns the distance between point1 and point2\n",
    "    where each argument is an array \n",
    "    consisting of the coordinates of the point\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def all_distances(training, new_point):\n",
    "    \"\"\"Returns an array of distances\n",
    "    between each point in the training set\n",
    "    and the new point (which is a row of attributes)\"\"\"\n",
    "    attributes = training.drop('Class')\n",
    "    def distance_from_point(row):\n",
    "        return distance(np.array(new_point), np.array(row))\n",
    "    return attributes.apply(distance_from_point)\n",
    "\n",
    "def table_with_distances(training, new_point):\n",
    "    \"\"\"Augments the training table \n",
    "    with a column of distances from new_point\"\"\"\n",
    "    return training.with_column('Distance', all_distances(training, new_point))\n",
    "\n",
    "def closest(training, new_point, k):\n",
    "    \"\"\"Returns a table of the k rows of the augmented table\n",
    "    corresponding to the k smallest distances\"\"\"\n",
    "    with_dists = table_with_distances(training, new_point)\n",
    "    sorted_by_distance = with_dists.sort('Distance')\n",
    "    topk = sorted_by_distance.take(np.arange(k))\n",
    "    return topk\n",
    "# ___________________________________\n",
    "# STEP 2:\n",
    "def majority(topkclasses):\n",
    "    ones = topkclasses.where('Class', are.equal_to(1)).num_rows\n",
    "    zeros = topkclasses.where('Class', are.equal_to(0)).num_rows\n",
    "    if ones > zeros:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def classify(training, new_point, k):\n",
    "    closestk = closest(training, new_point, k)\n",
    "    topkclasses = closestk.select('Class')\n",
    "    return majority(topkclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.5 The Accuracy of the Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. For 17.2 why do you randomly sample half for testing and half for training? How does this give you the accuracy of your classification method since you are reducing the data by half?\n",
    "2. Why is a training set not like a random sample from the population?\n",
    "3. What is training?\n",
    "4. What is testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
